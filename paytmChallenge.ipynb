{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sqlC = new org.apache.spark.sql.SQLContext(sc)\n",
    "import scala.util.matching.Regex\n",
    "import sqlC.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "import java.sql.Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/*\n",
    "Implicit safe conversions from string to other numeric data types, and to java.sql.Timestamp.\n",
    "*/\n",
    "implicit class StringConversion(val s: String) {\n",
    "\n",
    "  private def toTypeOrElse[T](convert: String=>T, defaultVal: T) = try {\n",
    "    if (s matches \"[\\\\+\\\\-0-9.e]+\") convert(s)\n",
    "    else defaultVal\n",
    "  } catch {\n",
    "    case _: NumberFormatException => defaultVal\n",
    "  }\n",
    "\n",
    "  def toShortOrElse(defaultVal: Short = 0) = toTypeOrElse[Short](_.toShort, defaultVal)\n",
    "  def toByteOrElse(defaultVal: Byte = 0) = toTypeOrElse[Byte](_.toByte, defaultVal)\n",
    "  def toIntOrElse(defaultVal: Int = 0) = toTypeOrElse[Int](_.toInt, defaultVal)\n",
    "  def toDoubleOrElse(defaultVal: Double = 0D) = toTypeOrElse[Double](_.toDouble, defaultVal)\n",
    "  def toLongOrElse(defaultVal: Long = 0L) = toTypeOrElse[Long](_.toLong, defaultVal)\n",
    "  def toFloatOrElse(defaultVal: Float = 0F) = toTypeOrElse[Float](_.toFloat, defaultVal)\n",
    "  \n",
    "  val defaultTimestamp = new Timestamp(0)\n",
    "  def toTimestampOrElse(defaultVal: Timestamp = defaultTimestamp) = try {\n",
    "      Timestamp.valueOf(s.replace(\"T\",\" \").replace(\"Z\", \" \"))\n",
    "  } catch {\n",
    "      case _: IllegalArgumentException => defaultTimestamp\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val rawRDD = sc.textFile(\"/resources/data/2015_07_22_mktplace_shop_web_log_sample.log.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1158500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawRDD.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Cleaning the Data\n",
    "\n",
    "A log record is a string that can look as the following. My first thought was to use Regex to split by any space that are not in the quotes. But it turned out to be a time consuming step. Instead, I splited them by quotes. A typical record will be splited into 5 parts. Some logs will be splited differently, for example the following string, as it has quotes inside the quotes. These records (total 22 records) are filtered out in the analysis. \n",
    ">Array(2015-07-22T16:10:38.028609Z marketpalce-shop 106.51.132.54:4841 10.0.4.227:80 0.000022 0.000989 0.00002 400 400 0 166 \"GET https://paytm.com:443/'\"\\'\\\");|]*{%0d%0a<%00>/about/ HTTP/1.1\" \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)\" DHE-RSA-AES128-SHA TLSv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val cleanRawRDD = rawRDD.filter(_.split(\"\\\"\").size == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1158478"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanRawRDD.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// split each line to X parts\n",
    "// Can be replaced with the regex \"( (?=([^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$))\", however will be much slower.\n",
    "def splitLine(line: String) = {\n",
    "    val firstSplit = line.split(\"\\\"\").filter(_ != \" \")\n",
    "    val firstPart = firstSplit(0).trim.split(\" \")\n",
    "    val lastPart = firstSplit(3).trim.split(\" \")\n",
    "    (firstPart :+ firstSplit(1) :+ firstSplit(2)) ++ lastPart\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val splitRDD = cleanRawRDD.map(line => splitLine(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val cleanSplitRDD = splitRDD.filter(line => line.size == 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1158478"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanSplitRDD.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// represents a log line\n",
    "// will be used as the schema of the spark DataFrame.\n",
    "case class WeblogRecord(\n",
    "    timestamp: java.sql.Timestamp,\n",
    "    elb: String,\n",
    "    clientIP: String,\n",
    "    clientPort: Int,\n",
    "    backendIPPort: String,\n",
    "    requestProcTime: Double,\n",
    "    backendProcTime: Double,\n",
    "    responseProcTime: Double,\n",
    "    elbStatus: Int,\n",
    "    backendStatus: Int,\n",
    "    recvBytes: Int,\n",
    "    sendBytes: Int,\n",
    "    reqMethod: String,\n",
    "    reqURL: String,\n",
    "    reqVer: String,\n",
    "    userAgent: String,\n",
    "    sslCipher: String,\n",
    "    sslProt: String\n",
    ")\n",
    "\n",
    "// convert from the splitted line to an instance of WebLogRecord, using the safe conversions defined above\n",
    "def rowToWeblogRecord(row: Array[String]) = {\n",
    "    val urlBreak = row(11).replace(\"\\\"\", \"\").split(\" \")\n",
    "    val clientBreak = row(2).split(\":\")\n",
    "    WeblogRecord(\n",
    "        row(0).toTimestampOrElse(),\n",
    "        row(1),\n",
    "        clientBreak(0),\n",
    "        clientBreak(0).toIntOrElse(-1),\n",
    "        row(3),\n",
    "        row(4).toDoubleOrElse(-1.0),\n",
    "        row(5).toDoubleOrElse(-1.0),\n",
    "        row(6).toDoubleOrElse(-1.0),\n",
    "        row(7).toIntOrElse(-1),\n",
    "        row(8).toIntOrElse(-1),\n",
    "        row(9).toIntOrElse(-1),\n",
    "        row(10).toIntOrElse(-1),\n",
    "        urlBreak.lift(0).getOrElse(\"\"),\n",
    "        urlBreak.lift(1).getOrElse(\"\"),\n",
    "        urlBreak.lift(2).getOrElse(\"\"),\n",
    "        row(12),\n",
    "        row(13),\n",
    "        row(14)\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val logDF = splitRDD.map(row => rowToWeblogRecord(row)).toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- elb: string (nullable = true)\n",
      " |-- clientIP: string (nullable = true)\n",
      " |-- clientPort: integer (nullable = false)\n",
      " |-- backendIPPort: string (nullable = true)\n",
      " |-- requestProcTime: double (nullable = false)\n",
      " |-- backendProcTime: double (nullable = false)\n",
      " |-- responseProcTime: double (nullable = false)\n",
      " |-- elbStatus: integer (nullable = false)\n",
      " |-- backendStatus: integer (nullable = false)\n",
      " |-- recvBytes: integer (nullable = false)\n",
      " |-- sendBytes: integer (nullable = false)\n",
      " |-- reqMethod: string (nullable = true)\n",
      " |-- reqURL: string (nullable = true)\n",
      " |-- reqVer: string (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- sslCipher: string (nullable = true)\n",
      " |-- sslProt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Sessional Page Hits\n",
    "For this analysis, a session is defined as all queries made by a user in a particular time period (**time window**). The following table is showing sessions in the time window of 30 minutes. It is equivalent to create a temporary view and run the following query in SQL:\n",
    "```python\n",
    "logDF.registerTempTable(\"webLog\")\n",
    "%%DataFrame\n",
    "sqlC.sql(\"\n",
    "    SELECT floor((unix_timestamp(timestamp)-1437532806)/(60*30)) timewin, clientIP, \n",
    "    COUNT(*) page_hits FROM webLog GROUP BY floor((unix_timestamp(timestamp)-1437532806)/(60*30)), clientIP \n",
    "    ORDER BY timewin, clientIP\"\n",
    ")\n",
    "```\n",
    "To generate the desired time window, an original time point has to be determined. The original time point is the minimum timestamp of the whole dataset. A time window rank number will be determined as: *(current timestamp - minimum timestamp)/(60 sec x window width in min)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1437532806]\n"
     ]
    }
   ],
   "source": [
    "/*\n",
    "calculate minimum timestamp (unix style) to subtract it from all the timestamps,\n",
    "so zero represents the beginning of the analyzed period\n",
    "*/\n",
    "val min_timestamp = logDF.select(min(unix_timestamp($\"timestamp\"))).collect()\n",
    "println(min_timestamp(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----+\n",
      "|timewin|     clientIP|count|\n",
      "+-------+-------------+-----+\n",
      "|      0| 1.186.247.60|    4|\n",
      "|      0|  1.186.41.10|    3|\n",
      "|      0|  1.186.44.42|    1|\n",
      "|      0|  1.186.78.17|    4|\n",
      "|      0|   1.186.78.9|    9|\n",
      "|      0| 1.187.140.24|    1|\n",
      "|      0|1.187.164.121|    2|\n",
      "|      0| 1.187.164.29|    9|\n",
      "|      0|1.187.168.128|    4|\n",
      "|      0|1.187.186.198|    4|\n",
      "|      0| 1.187.187.45|    1|\n",
      "|      0| 1.187.193.22|    1|\n",
      "|      0| 1.187.202.23|    1|\n",
      "|      0|  1.187.208.2|    6|\n",
      "|      0| 1.187.233.50|    3|\n",
      "|      0|1.187.249.228|   22|\n",
      "|      0| 1.187.250.28|    4|\n",
      "|      0| 1.187.250.52|    7|\n",
      "|      0|  1.187.43.74|    3|\n",
      "|      0|  1.187.49.35|    1|\n",
      "+-------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logDF.select(floor((unix_timestamp($\"timestamp\")-1437532806)/(60*30)).as(\"timewin\"), $\"clientIP\").groupBy($\"timewin\", $\"clientIP\").count().orderBy($\"timewin\", $\"clientIP\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Average Session Time\n",
    "To calculate the average session time, the time span of each session is needed. The time span of each session (in seconds) is defined as: _the last query time - the first query time_ during each session. For clarity purpose, the following table is defined as *timespanDF_30* for the subsequent calculation of the total average session time. My result shows that the users spend an average time of __99.46__ seconds per session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------+\n",
      "|timewin|       clientIP|timespan|\n",
      "+-------+---------------+--------+\n",
      "|      0| 101.60.167.204|      76|\n",
      "|      0| 117.253.105.27|      30|\n",
      "|      0| 106.76.176.223|       1|\n",
      "|      0|  117.249.186.1|       0|\n",
      "|      0|107.167.107.125|       0|\n",
      "|      0|112.133.215.242|      27|\n",
      "|      0|107.167.107.120|     199|\n",
      "|      0| 136.185.171.11|       0|\n",
      "|      0|   49.248.85.79|     243|\n",
      "|      0|180.151.208.141|       5|\n",
      "|      0|  117.244.52.67|       1|\n",
      "|      0|    1.39.46.165|      35|\n",
      "|      0|  183.82.147.94|       2|\n",
      "|      0|117.255.253.154|       0|\n",
      "|      0|175.100.175.178|       0|\n",
      "|      0|  43.230.38.170|       0|\n",
      "|      0|   117.203.8.98|      16|\n",
      "|      0|  37.228.105.65|      67|\n",
      "|      0|  117.196.56.70|     178|\n",
      "|      0|  60.243.144.23|     127|\n",
      "+-------+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logDF.select(floor((unix_timestamp($\"timestamp\")-1437532806)/(60*30)).as(\"timewin\"), $\"clientIP\", $\"timestamp\").groupBy($\"timewin\", $\"clientIP\").agg((max(unix_timestamp($\"timestamp\"))-min(unix_timestamp($\"timestamp\"))).alias(\"timespan\")).orderBy($\"timewin\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val timespanDF_30=logDF.select(floor((unix_timestamp($\"timestamp\")-1437532806)/(60*30)).as(\"timewin\"), $\"clientIP\", $\"timestamp\").groupBy($\"timewin\", $\"clientIP\").agg((max(unix_timestamp($\"timestamp\"))-min(unix_timestamp($\"timestamp\"))).alias(\"timespan\")).orderBy($\"timewin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+-----------------+\n",
      "|    avg(timespan)|\n",
      "+-----------------+\n",
      "|99.46373757313842|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timespanDF_30.select(avg($\"timespan\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Time Window\n",
    "How to define time window is often a question of debates. For this particular dataset, I tried a time window of 15, 30 and 60 minutes and assessed their effects on the average time span per time window. It is expected that larger time window will capture longer session, resulting the increase of the average time span. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val timespanDF_15=logDF.select(floor((unix_timestamp($\"timestamp\")-1437532806)/(60*15)).as(\"timewin\"), $\"clientIP\", $\"timestamp\").groupBy($\"timewin\", $\"clientIP\").agg((max(unix_timestamp($\"timestamp\"))-min(unix_timestamp($\"timestamp\"))).alias(\"timespan\")).orderBy($\"timewin\")\n",
    "val timespanDF_30=logDF.select(floor((unix_timestamp($\"timestamp\")-1437532806)/(60*30)).as(\"timewin\"), $\"clientIP\", $\"timestamp\").groupBy($\"timewin\", $\"clientIP\").agg((max(unix_timestamp($\"timestamp\"))-min(unix_timestamp($\"timestamp\"))).alias(\"timespan\")).orderBy($\"timewin\")\n",
    "val timespanDF_60=logDF.select(floor((unix_timestamp($\"timestamp\")-1437532806)/(60*60)).as(\"timewin\"), $\"clientIP\", $\"timestamp\").groupBy($\"timewin\", $\"clientIP\").agg((max(unix_timestamp($\"timestamp\"))-min(unix_timestamp($\"timestamp\"))).alias(\"timespan\")).orderBy($\"timewin\")\n",
    "\n",
    "val avgTimeSpanDF_15 = timespanDF_15.select($\"timewin\", $\"timespan\").groupBy($\"timewin\").agg(avg($\"timespan\"))\n",
    "val avgTimeSpanDF_30 = timespanDF_30.select($\"timewin\", $\"timespan\").groupBy($\"timewin\").agg(avg($\"timespan\"))\n",
    "val avgTimeSpanDF_60 = timespanDF_60.select($\"timewin\", $\"timespan\").groupBy($\"timewin\").agg(avg($\"timespan\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|timewin|     avg(timespan)|\n",
      "+-------+------------------+\n",
      "|      0| 41.39601353892441|\n",
      "|      9|               0.0|\n",
      "|     10|49.036207717960934|\n",
      "|     16|               0.0|\n",
      "|     17| 49.87924071082391|\n",
      "|     25| 53.48273578658594|\n",
      "|     31| 99.61538461538461|\n",
      "|     32|  57.4678470668353|\n",
      "|     33|56.532908089301245|\n",
      "|     37|            36.125|\n",
      "|     41|49.166666666666664|\n",
      "|     42|               7.0|\n",
      "|     52|              31.5|\n",
      "|     53|               0.0|\n",
      "|     54|110.76200461312644|\n",
      "|     55|               0.0|\n",
      "|     56| 54.85549189084199|\n",
      "|     59|               0.0|\n",
      "|     60| 58.58223583460949|\n",
      "|     61| 49.28644314868804|\n",
      "|     67|             149.5|\n",
      "|     73| 45.15597569209993|\n",
      "|     74|            4.1625|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avgTimeSpanDF_15.show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|timewin|     avg(timespan)|\n",
      "+-------+------------------+\n",
      "|      0| 41.39601353892441|\n",
      "|      4|               0.0|\n",
      "|      5|49.036207717960934|\n",
      "|      8|  49.9294656077534|\n",
      "|     12| 53.48273578658594|\n",
      "|     15| 99.61538461538461|\n",
      "|     16| 153.1666307393835|\n",
      "|     18|            36.125|\n",
      "|     20|49.166666666666664|\n",
      "|     21|               7.0|\n",
      "|     26| 32.18181818181818|\n",
      "|     27|110.85610945117156|\n",
      "|     28| 54.85549189084199|\n",
      "|     29|               0.0|\n",
      "|     30|136.73887355052457|\n",
      "|     33|             149.5|\n",
      "|     36| 45.15597569209993|\n",
      "|     37|            4.1625|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avgTimeSpanDF_30.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the average time span in 15-min and 30-min time windows (above 2 tables), I noticed the values in the 30-min windows of 10:40-11:10 (*16th*) and 17:40-18:10 (*30th*) are much larger than in corresponding 15-min time windows, suggesting users during these time periods had more engaging sessions (>15 min) and thus the 30-min time window is more suitable for defining the sessions. \n",
    "\n",
    "\n",
    "The following table shows the average time span in 60-min time windows. Extending time window to 60 minutes did not change the average time span of sessions, indicating the users were all engaging under 30 minutes during a session. As a result, the 30-min time window is the optimal among the three time windows tested. Therefore, my subsequent analysis will only be using 30-min time window for sessionization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|timewin|     avg(timespan)|\n",
      "+-------+------------------+\n",
      "|      0| 41.39601353892441|\n",
      "|      2| 49.06876290296967|\n",
      "|      4|  49.9294656077534|\n",
      "|      6| 53.48273578658594|\n",
      "|      7| 99.61538461538461|\n",
      "|      8| 153.1666307393835|\n",
      "|      9|            36.125|\n",
      "|     10|             50.25|\n",
      "|     13|110.94042441708147|\n",
      "|     14|  55.1799378520101|\n",
      "|     15|136.73887355052457|\n",
      "|     16|             149.5|\n",
      "|     18| 48.20437017994858|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avgTimeSpanDF_60.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Sessional Unique URL\n",
    "unique URL per session can be easily calculated by counting distinct url in each session. The following table shows unique URL counts in 30-min time windows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+-------+-------------+---------+\n",
      "|timewin|     clientIP|uniqueURL|\n",
      "+-------+-------------+---------+\n",
      "|      0| 1.186.247.60|        4|\n",
      "|      0|  1.186.41.10|        3|\n",
      "|      0|  1.186.44.42|        1|\n",
      "|      0|  1.186.78.17|        4|\n",
      "|      0|   1.186.78.9|        7|\n",
      "|      0| 1.187.140.24|        1|\n",
      "|      0|1.187.164.121|        2|\n",
      "|      0| 1.187.164.29|        8|\n",
      "|      0|1.187.168.128|        4|\n",
      "|      0|1.187.186.198|        4|\n",
      "|      0| 1.187.187.45|        1|\n",
      "|      0| 1.187.193.22|        1|\n",
      "|      0| 1.187.202.23|        1|\n",
      "|      0|  1.187.208.2|        3|\n",
      "|      0| 1.187.233.50|        3|\n",
      "|      0|1.187.249.228|        8|\n",
      "|      0| 1.187.250.28|        4|\n",
      "|      0| 1.187.250.52|        6|\n",
      "|      0|  1.187.43.74|        3|\n",
      "|      0|  1.187.49.35|        1|\n",
      "+-------+-------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logDF.select(floor((unix_timestamp($\"timestamp\")-1437532806)/(60*30)).as(\"timewin\"), $\"clientIP\", $\"reqURL\").groupBy($\"timewin\", $\"clientIP\").agg(countDistinct($\"reqURL\").alias(\"uniqueURL\")).orderBy($\"timewin\", $\"clientIP\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Most Engaged Users\n",
    "Most engaged users are IPs/users with the longest session times. The first table contains users with longest session times in a 30-min time window. The second table contains users with largest sum of session times from all time windows. Insterestingly, the top four users in both tables are the same users, confirming these users are genuine most engaged users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+-------+---------------+--------+\n",
      "|timewin|       clientIP|timespan|\n",
      "+-------+---------------+--------+\n",
      "|     30|  220.226.206.7|    1505|\n",
      "|     30|   52.74.219.71|    1499|\n",
      "|     30|  119.81.61.166|    1499|\n",
      "|     30|  182.48.232.41|    1498|\n",
      "|     30|  54.251.151.39|    1497|\n",
      "|     30|   46.236.24.53|    1496|\n",
      "|     30| 182.18.177.214|    1496|\n",
      "|     30| 180.179.213.70|    1494|\n",
      "|     30|  54.169.191.85|    1494|\n",
      "|     30| 122.248.183.22|    1493|\n",
      "|     30|  49.204.55.206|    1492|\n",
      "|     30|  66.249.71.110|    1492|\n",
      "|     30|   49.205.90.93|    1492|\n",
      "|     30|168.235.197.212|    1492|\n",
      "|     30|   103.42.88.34|    1490|\n",
      "|     30|  66.249.71.118|    1490|\n",
      "|     30|  54.244.52.204|    1490|\n",
      "|     30|117.207.121.176|    1489|\n",
      "|     30|   66.102.6.250|    1488|\n",
      "|     30|   54.228.16.12|    1488|\n",
      "+-------+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timespanDF_30.orderBy(desc(\"timespan\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+--------------+------------+\n",
      "|      clientIP|sum_timespan|\n",
      "+--------------+------------+\n",
      "| 220.226.206.7|        6426|\n",
      "|  52.74.219.71|        5817|\n",
      "| 119.81.61.166|        5808|\n",
      "| 54.251.151.39|        5787|\n",
      "| 106.186.23.95|        5555|\n",
      "|121.58.175.128|        5383|\n",
      "|  125.19.44.66|        5233|\n",
      "| 54.169.191.85|        5176|\n",
      "|180.179.213.94|        5067|\n",
      "| 54.244.52.204|        5033|\n",
      "|54.250.253.236|        5008|\n",
      "|180.179.213.70|        4991|\n",
      "|54.252.254.204|        4926|\n",
      "|  207.46.13.22|        4914|\n",
      "|180.179.213.71|        4879|\n",
      "|122.252.231.14|        4751|\n",
      "|176.34.159.236|        4748|\n",
      "| 54.245.168.44|        4723|\n",
      "| 54.243.31.236|        4718|\n",
      "| 54.251.31.140|        4609|\n",
      "+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timespanDF_30.select($\"clientIP\", $\"timespan\").groupBy($\"clientIP\").agg(sum($\"timespan\").alias(\"sum_timespan\")).orderBy(desc(\"sum_timespan\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Distinct User Identification\n",
    "IP does not guarantee distinct users, as several users can share the same external IP address.\n",
    "Based on the available data, we can use the client user-agent, combined with the IP address, to identify a distinct user.\n",
    "While this method still does not guarantee 100% that the identified users are distinct, as several users with the same IP can have the same user-agent and the same user can use multiple user-agents (e.g., if they use multiple browsers), it does provide a way to improve the identification process.\n",
    "\n",
    "Following is the number of distinct users based only on IP address, and the number of distinct users based on both IP address and the user agent. The latter method manages to distinguish more distinct users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([90544])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logDF.agg(countDistinct($\"clientIP\")).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([112594])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logDF.agg(countDistinct($\"clientIP\", $\"userAgent\")).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table counts the number of user agent per IP, clearly showing that indeed a same IP address can have as many as 49 different user agents, indicating the significance of this method in furthur identifying distinct users per IP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+---------------+-------------+\n",
      "|       clientIP|cntUserAgents|\n",
      "+---------------+-------------+\n",
      "|   125.19.44.66|           49|\n",
      "|  116.50.59.180|           31|\n",
      "| 122.252.231.14|           31|\n",
      "|   59.144.58.37|           30|\n",
      "|  123.63.74.210|           28|\n",
      "|168.235.197.212|           26|\n",
      "| 203.143.186.45|           25|\n",
      "|   125.20.9.248|           24|\n",
      "| 117.239.35.226|           23|\n",
      "|   112.121.55.9|           23|\n",
      "|168.235.197.151|           22|\n",
      "|203.145.131.164|           21|\n",
      "|   117.239.53.7|           20|\n",
      "|   103.12.119.2|           20|\n",
      "| 103.22.172.130|           19|\n",
      "|168.235.197.149|           19|\n",
      "| 115.111.223.43|           19|\n",
      "|    61.246.57.5|           19|\n",
      "|  14.139.241.84|           19|\n",
      "|168.235.197.238|           19|\n",
      "+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logDF.groupBy($\"clientIP\").agg(countDistinct($\"userAgent\") as \"cntUserAgents\").orderBy(desc(\"cntUserAgents\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
